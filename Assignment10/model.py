# -*- coding: utf-8 -*-
"""Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X89qdfO6VI9MCo1DGFLQcZHznZCA1e18
"""
import torch.nn as nn
import torch.nn.functional as F

class ResNet(nn.Module):
    def __init__(self):
        super(ResNet, self).__init__()\

        #sequence
###########################       Preparation Layer  ######################################################################################################
        # Input Block
        self.prepblock = nn.Sequential(
            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(3, 3), padding=1, stride=1, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(),
        ) # output_size = 30
###########################       Layer 1  ######################################################################################################
        self.convblock1 = nn.Sequential(
            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), padding=1, stride=1, bias=False),
            nn.MaxPool2d(2, 2) ,# output_size =
            nn.BatchNorm2d(128),
            nn.ReLU(),
        )
       # output size

        # CONVOLUTION BLOCK 2
        self.resnet1 = nn.Sequential(
            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.ReLU()

        ) # output_size =

###########################       Layer 2  ######################################################################################################
        self.convblock2 = nn.Sequential(
            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(3, 3), padding=1, stride=1, bias=False),
            nn.MaxPool2d(2, 2) ,# output_size =
            nn.BatchNorm2d(256),
            nn.ReLU(),
        )
       # output size
###########################       Layer 3  ######################################################################################################
        self.convblock3 = nn.Sequential(
            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=(3, 3), padding=1, stride=1, bias=False),
            nn.MaxPool2d(2, 2) ,# output_size =
            nn.BatchNorm2d(512),
            nn.ReLU(),
        )
             # CONVOLUTION BLOCK 2
        self.resnet2 = nn.Sequential(
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), padding=1, bias=False),
            nn.BatchNorm2d(512),
            nn.ReLU(),
            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=(3, 3), padding=1, bias=False),
            nn.BatchNorm2d(512),
            nn.ReLU()

        ) # output_size =
        self.pool1 = nn.MaxPool2d(4, 4) # output_size =
        self.seq1 = nn.Sequential(
            nn.Conv2d(in_channels=512, out_channels=10, kernel_size=(1, 1), padding=0, bias=False),
            #nn.AvgPool2d(5)
            # nn.BatchNorm2d(10), NEVER
            # nn.ReLU() NEVER!
        )
    def forward(self, x):
        # Prep Block
        x = self.prepblock(x)
      #  print ("Prep Size: ",x.size())
        # Layer 1
        x = self.convblock1(x)
       # print ("conv block layer 1: ",x.size(), x.shape)
        x1 = self.resnet1(x)
        #print ("conv block x1: ",x.size(), x.shape)
        x = x + x1
        #print ("conv block add: ",x.size())

        # Layer 2
        x = self.convblock2(x)
        #print ("conv block layer 2: ",x.size())

        # Layer 3
        x = self.convblock3(x)
        #print ("conv block layer 3: ",x.size(), x.shape)
        x1 = self.resnet2(x)
        #print ("conv block x1: ",x.size(), x.shape)
        x = x + x1
        #print ("conv block add: ",x.size())
     #   x = self.convblock2(x)
        x = self.pool1(x)
        #print ("pool : ",x.size())
        x = self.seq1(x)
      #  result = torch.add(x, x1)
        #print ("conv block add: ",x.size())
  #      print(result.size())
        x = x.view(-1, 10)
        return F.log_softmax(x, dim=-1)

